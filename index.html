<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance" />
  <meta property=" og:title" content="Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance" />
  <meta property="og:description" content="Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance" />
  <meta property="og:url" content="https://raven-lm.github.io/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance">
  <meta name="twitter:description" content="Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="language models, natural language generation, specialized domains, error propagation, hallucination, reasoning across heterogeneous datasets, precision, finance, language model augmentation, external tools, supervised fine-tuning, LLaMA-2 13B Chat model, task router, task solver, question-answering datasets, financial domain, raven, baseline improvement, GPT 3.5 CoT, in-context learning. baselines">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/crow-solid.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/deb78776bf.js" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/772f25de98.js" crossorigin="anonymous"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Equipping Language Models with Tool Use Capability for Tabular Data
              Analysis in Finance</h1>
            <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Adrian Theuma and
              </span>
              <span class="author-block">
                <a href="https://eehsan.github.io" target="_blank">Ehsan Shareghi</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Monash University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary Model link -->
                <span class="link-block">
                  <a href="https://huggingface.co/adriantheuma/raven-lora" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-regular fa-box-archive"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- Supplementary data link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/adriantheuma/raven-data" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/adriantheuma/llama2-raven" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- App -->
                <span class="link-block">
                  <a href="https://forms.gle/oKNXcohVyXVHxb9S9" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-solid fa-crow"></i>
                    </span>
                    <span>Raven App</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="item has-text-centered">
        <img src="static/images/raven-demo.gif" alt="Raven gif animation" width="95%">
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          While language model fine-tuning and tool use are both popular topics, their use in specialised domains such
          as finance remains understudied. This study initiates the exploration of numerous benefits associated with
          fine-tuning Language
          Models (LMs) for tool use in finance.
        </h2>
        <h2 class="subtitle has-text-centered">
          Through a series of experiments we demonstrate the feasibility of
          extending a small scale open foundation
          model into the financial domain. We obtain a new model,
          Raven, by
          leveraging <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">Llama 2 13B chat</a> and fine-tune
          a mere 0.2% of
          its
          parameters.
        </h2>
        <h2 class="subtitle has-text-centered">
          Raven remarkably elevates the performance of the base model from 2% to 56.7% on a table and text
          arithmetic question answering benchmark. On average, across the
          datasets we
          experimented with, Raven lifts average absolute performance by 35.2%
          compared
          to the base model, surpassing even a significantly larger <a
            href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> model by 9.2%!
        </h2>
        <h3>
          Fill <a href="https://forms.gle/oKNXcohVyXVHxb9S9">this form</a> if you wish to express interest in using the
          app.
        </h3>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Despite impressive natural language generation capabilities exhibited by large language models (LLMs),
              their applicability in specialised domains is hindered by error propagation, hallucination, and the
              challenge of reasoning across heterogeneous datasets. These limitations pose significant obstacles in
              domains where precision is paramount, such as health and finance. We explore language model augmentation
              with external tools, which has shown promise in mitigating these limitations, by offloading specific steps
              of the reasoning process to external tools better equipped for the task.
              More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a
              LLaMA-2 13B Chat model to act both as a <em>task router</em> and <em>task solver</em>. The <em>task
                router</em> dynamically directs a question to either be answered internally by the LLM or externally via
              the
              right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of
              35.2% over the baseline, and is competitive with strong <a
                href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> CoT and in-context learning
              baselines.
            </p>
          </div>
        </div>
      </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method -->
  <section class="section" id="method">
    <div class="container is-max-desktop content">
      <h2 class="title">Method</h2>
      <p>
        Inspired by the success of the <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a>
        instruction-tuning protocol we use the
        instruction-tuned <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">Llama 2 13B chat</a> language
        model and
        fine-tune it further using the <a href="https://huggingface.co/docs/peft/conceptual_guides/lora">LoRA</a>
        lightweight fine-tuning
        approach. For training we use a <a href="https://drive.google.com/file/d/1J4UqjGDZEFqRPKsug3mC-OcXkWLCSAe5">set
          of financial and generic structured and unstructured open-domain datasets</a> to
        obtain a tool-augmented instruction following language model for finance, which we call Raven.
      </p>
      <div class="item has-text-centered">
        <img src="static/images/raven-infeerence-pipeline.png" alt="Teaser image" class="teaser-image" width="90%">
      </div>

      <p>
        <strong>Inference pipeline</strong>. We use different prompt templates to <em>steer</em> Raven to either produce
        the final answer or produce an intermediate result, which is subsequently interpreted by one of the external
        tools.
      </p>
      <p>
        <strong>Task-router.</strong> At inference time we prompt Raven twice for every query.
        Initially we
        wrap the prompt in a specialised <em>'template choice'</em> prompt and expect the model to predict the best
        prompt
        template to use from <em>'arithmetic'</em>, <em>'classification'</em>, <em>'script'</em> or <em>'information
          extraction'</em>,
        conditioned on the input.
      </p>
      <p>
        <strong>Task-solver.</strong> The instruction, including the input and data if applicable, are wrapped in the
        prompt template that the model inferred and this is sent to Raven to generate the next output.
        Depending on the selected template the <em>task router</em> then invokes a tool to fulfill the request, or
        produces the response directly.
      </p>

    </div>
  </section>

  <section class="section" id="tools">
    <div class="container is-max-desktop content">
      <h2 class="title">Tools</h2>
      Raven offloads nuanced structured
      data extraction and arithmetic expression calculation to two external deterministic tools: a calculator and a
      database engine.
      <p>
        <strong>Calculator API.</strong> A calculator is instantiated in a python interpreter and is used to evaluate
        well-formed arithmetic expressions. The API expects one parameter representing the arithmetic expression and
        returns the evaluated result.
      </p>
      <p>
        <strong>Lightweight database engine API.</strong> We create
        an API capable of executing SQL scripts on relational data. The API expects two parameters, (1) a string
        representation of the structured data in JSON format and (2) a SQL script. The API's internal lightweight
        database
        engine converts structured data from its textual form to the engine's internal relational representation and
        converts data types where applicable. The SQL script is executed on this internal representation and the API
        returns the resulting execution result.
      </p>

    </div>
  </section>


  <!--Result-->
  <section class="section" id="">
    <div class="container is-max-desktop content">
      <h2 class="title">Results</h2>
      <!DOCTYPE html>
      <html>

      <head>
        <style>
          table {
            border-collapse: collapse;
            width: 100%;
          }

          th,
          td {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 8px;
          }

          th {
            background-color: #f2f2f2;
          }
        </style>
      </head>

      <body>
        <table class="table is-hoverable">
          <thead>
            <tr class="header">
              <th></th>
              <th colspan="4" ;align="center">Dataset</th>
            </tr>
            <tr class="header">
              <th>Model</th>
              <th>OTT-QA</th>
              <th>TAT-QA</th>
              <th>Wiki-SQL</th>
              <th>PhraseBank</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT 3.5 (CoT)</td>
              <td>5.55%</td>
              <td>19.23%</td>
              <td>32.07%</td>
              <td>44.18%</td>
            </tr>
            <tr>
              <td>GPT 3.5 (5-Shot)</td>
              <td>14.55%</td>
              <td>34.06%</td>
              <td>53.00%</td>
              <td>70.07%</td>
            </tr>
            <tr>
              <td>&nbsp;&nbsp;+Tools</td>
              <td>14.60%</td>
              <td>46.82%</td>
              <td>75.88%</td>
              <td>71.73%</td>
            </tr>
            <tr>
              <td>Llama 2 13B chat</td>
              <td>6.18%</td>
              <td>10.91%</td>
              <td>21.68%</td>
              <td>66.03%</td>
            </tr>
            <tr>
              <td>&nbsp;&nbsp;+SFT</td>
              <td><strong>20.10%</strong></td>
              <td>37.87%</td>
              <td>74.38%</td>
              <td>90.97%</td>
            </tr>
            <tr>
              <td>Raven</td>
              <td>16.03%</td>
              <td>51.35%</td>
              <td>84.25%</td>
              <td><strong>91.92</strong>%</td>
            </tr>
            <tr>
              <td>&nbsp;&nbsp;+Backoff</td>
              <td>16.03%</td>
              <td><strong>52.27%</strong></td>
              <td><strong>85.52%</strong></td>
              <td>91.92%</td>
            </tr>
          </tbody>
        </table>

        <p><strong>Fine-tuning and using tools significantly improves performance.</strong> When compared to the base
          model Raven significantly improves the results on the <a
            href="https://paperswithcode.com/sota/sentiment-analysis-on-financial-phrasebank">PhraseBank</a>
          dataset by an absolute 25.9%. These encouraging results suggest that
          even
          a limited dataset can endow a pre-trained language model with a sophisticated understanding of financial
          language, obtaining high accuracy in a financial sentiment classification benchmark.
        </p>
        <p>
          On the <a href="https://paperswithcode.com/dataset/wikisql">Wiki-SQL</a> dataset the base model is unable to
          infer the correct answer almost 80% of the
          time. This figure is inverted when the same benchmark is evaluated on Raven that obtains a
          <em>4-fold</em>> improvement over the base model inferring the correct answer more than 85% of the time. Our
          model
          improves on the best <a href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> performance by
          close to 10% (absolute). All the questions in this dataset
          can be addressed using the lightweight database engine and involve a combination of
          data selection, ranking and arithmetic operations on structured data. This result underscores the distinct
          advantage of delegating this task to a tool rather than relying on the language model to infer the results in
          a
          zero-shot manner. Despite the results not being as strong as Raven we observe a similar pattern on the
          <a href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> evaluation in which better results are
          incrementally obtained when including examples in the
          context and using tools compared to <a href="https://arxiv.org/abs/2205.11916">Zero-Shot-CoT</a>.
        </p>
        <p>
          We see a similar pattern on the <a href="https://paperswithcode.com/dataset/tat-qa">TAT-QA</a> benchmark with
          the tool augmented model achieving a
          <em>5-fold</em> improvement on the base model. Approximately 46% of the observations of the TAT-QA
          dataset are annotated with an intermediate arithmetic derivation that Raven evaluates using a
          calculator at inference time. In the analysis section we outline a comparative
          analysis to explore whether our model performs better on this partition of the data.
        </p>
        <p>
          The majority of questions in <a href="https://paperswithcode.com/dataset/ott-qa">OTT-QA</a> require multi-hop
          inference involving both tabular data and
          unstructured text, with the information needed to answer the questions dispersed differently across these two
          input types. The baseline model from <a href="https://arxiv.org/abs/2010.10439">Chen at. al.</a>, which
          employs an iterative retriever and
          a BERT-based reader, attains an exact match score of less than 10%. This dataset does not have annotated
          intermediate steps to get to the answer and therefore all models are expected to infer the answer in a
          zero-shot
          manner without using tools. Despite Raven achieving an increase in experimental accuracy compared to
          the base model the relatively low score underscores the importance of intermediate reasoning steps and tools.
        </p>
        <p>
          <strong>Backoff.</strong> We employ a backoff mechanism when Raven produces malformed
          expressions and fallback to the result obtained from the SFT model. This mechanism improves the results of
          datasets that employ tools at inference time: TAT-QA (51.35% &rarr; 52.27%) and
          Wiki-SQL (84.25% &rarr; 85.52%).
        </p>



    </div>
  </section>



  <!-- Example -->
  <section class="section" id="example">
    <div class="container is-max-desktop content">
      <h2 class="title">Analysis</h2>
      <!-- insert two figures data_scale and data_type side by side-->
      <div class="columns is-centered">
        <div style="width: 50%;">
          <div class="item has-text-centered">
            <img src="static/images/tat-qa_results.png" alt="Teaser image" class="teaser-image" width="90%">
          </div>
        </div>
        <div style="width: 50%;">
          <div class="item has-text-centered">
            <img src="static/images/tat-qa_results_complexity.png" alt="Teaser image" class="teaser-image" width="90%">
          </div>
        </div>
      </div>
      <p>
        <strong>One model or many?.</strong> To assess the performance penalty of mixing all the datasets and training
        one multi-tasking model, we also train a model exclusively on the TAT-QA dataset. The evaluation result
        for this dedicated model on the TAT-QA dataset is 54.70%, which is 2.4% higher than Raven. We
        contend that this modest performance gain does not justify the overhead associated with maintaining separate
        models and switching between them during inference.
      </p>
      <p>
        <strong>The effects of tool augmentation.</strong> Approximately half of the questions within the TAT-QA
        dataset are annotated with an arithmetic equation. The presence of the equation implies that the language model
        needs to perform multi-hop reasoning to output the correct answer. This process involves the correct extraction
        of, at a minimum, two numerical values from the context, followed by the execution of an arithmetic operation,
        such as addition or division. This particular scenario is ideal to understand the effect of SFT and tool
        augmentation, by comparing the performance of different models on the two categories of data from the same
        dataset.
      </p>
      <p>
        We observe that SFT models are able to accurately extract multiple data points from the context, but require
        external tools to correctly compose the final answer from the gathered data. As shown in the above Figure
        the base model without any fine-tuning is ill-equipped to perform multi-hop reasoning
        achieving close to 2% accuracy (blue column) equating to ten correct answers
        of approximately 620. Although we observe an improvement in the SFT model, the impact of using tools is evident
        in
        the substantial jump to 56.7% accuracy achieved by Raven. Additionally, the consistent performance of
        the 'Information Extraction' type questions between SFT and Raven (orange column), which only
        requires data extraction to answer the question, continuous to re-enforce this observation.
      </p>
      <p>
        The utility of augmenting language models with external tools is substantiated further through a comparative
        analysis of experimental outcomes on two similar datasets. Addressing questions on Wiki-SQL and
        OTT-QA requires multi-hop reasoning across diverse forms of data, spanning both structured and
        unstructured formats. The primary difference lies in the annotation method: the Wiki-SQL dataset is
        annotated with a data extraction script which, when executed on the structured data, yields the answer. In
        contrast, the OTT-QA dataset lacks this intermediate derivation step. By delegating the script execution
        to an external tool, Raven achieves an exact match accuracy of <em>85.52%</em> on Wiki-SQL and 16.03%
        on OTT-QA, underscoring the effectiveness of fit-for-purpose external tools in this scenario.
      </p>
      <p>
        <strong>Question complexity.</strong> On the TAT-QA dataset we can use the number of arithmetic
        operators in the <em>gold</em> evaluation as a proxy for question complexity. One arithmetic operator implies
        the
        extraction of two numerical values from the context, two operators, three numerical values, and so on. As shown
        in
        the above, model performance degrades with the number of operators (x-axis) and
        hence the numerical values required to be extracted from the context.
      </p>
    </div>
  </section>


  <section class="section" id="example">
    <div class="container is-max-desktop content">
      <h2 class="title">Limitations</h2>
      <p>
        <strong>Hardware bottleneck.</strong> Our experiments were constrained with fitting our model on available
        commodity
        hardware. We hypothesise that it would be possible to obtain better performance using the larger <a
          href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama 2 70
          billion-parameter model</a> and a longer context length.
      </p>
      <p>
        <strong>Language model evaluation.</strong> Our experimental
        results demonstrate that evaluating natural language generation models (NLG) in a domain characterised by the
        ubiquitous presence of numerical
        values, such as finance, poses a significant challenge. Mainstream NLG evaluation metrics provide a measure of
        similarity between generations which is reported in terms of surface form similarity. These measures are not
        suitable
        for comparing numerical content. For example the candidate and reference sentences <em>"The amount of goodwill
          reallocated to the IOTG operating segment in 2018 was $480 <strong>million</strong>"</em>, and <em>"The amount
          of
          goodwill reallocated to the IOTG operating segment in 2018 was $480"</em> have a <a
          href="https://huggingface.co/spaces/evaluate-metric/bertscore">BERTScore</a> of 99.17%!
        Conversely, using exact match criteria might unjustly penalise NLG models,
        given that
        identical numerical values can be expressed in varying forms - such as "$4 million" and "$4,000,000," or "0.24"
        and
        "24%,". In some cases, numerical values can be integrated within a passage of text, rendering the evaluation of
        such
        content very challenging.
      </p>
      <p>
        <strong>GPT-3.5 evaluation.</strong> Evaluating our benchmark with <a
          href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> poses significant challenges,
        particularly when using <a
          href="https://medium.com/@JerryCuomo/lets-think-step-by-step-advanced-reasoning-in-business-with-chain-of-thought-prompting-dd5ae8a6008#:~:text=In%20chain%2Dof%2Dthought%20prompting,bite%2Dsized%2C%20logical%20chunks.">chain-of-though</a>
        reasoning. GPT-3.5 does
        not consistently adhere to instructions for providing a concise response, such as a single word or number, which
        makes
        exact match comparisons challenging. Additionally, we have noticed that <a
          href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> does not generate a response
        when uncertain. This is particularly evident when evaluating the PhraseBank dataset, which does not exhibit
        common sentiment negative or positive words.
      </p>

    </div>
  </section>

  <section class="section" id="example">
    <div class="container is-max-desktop content">
      <h2 class="title">Conclusion</h2>
      <p>
        Through a series of experiments we have demonstrated the feasibility of extending a small scale open foundation
        model into the financial domain. We obtain a new model,
        Raven, by
        leveraging <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">Llama 2 13B chat</a> and use a
        relatively small and diverse dataset to fine-tune a mere 0.2% of
        its
        parameters. Raven remarkably elevates the performance of the base model from 2% to 56.7% on the
        TAT-QA arithmetic question answering benchmark. On average, across the
        datasets we
        experimented with, Raven lifts average absolute performance by 35.2%
        compared
        to the base model, surpassing even a significantly larger <a
          href="https://platform.openai.com/docs/models/gpt-3-5">GPT 3.5</a> model by 9.2%. Additionally, through a
        comparative
        analysis of question answering datasets we demonstrate the effectiveness of augmenting language models with
        external
        tools, showing significant improvements in accuracy when addressing multi-hop questions with tools.


      </p>
      <p>
        <strong>Future work.</strong> The financial domain encompasses not only structured and unstructured data but
        also the
        pervasive presence of visual representations of data in charts. To develop a comprehensive financial language
        model,
        it becomes imperative to endow these models with the capability to engage in multi-modal reasoning.
        Additionally, in our pursuit to create specialised domain AI assistants, our efforts should extend beyond
        enhancing
        their predictive capabilities. While it is undoubtedly crucial to push the boundaries and surpass previous
        state-of-the-art achievements, we believe it is equally imperative to equip language models with the capacity to
        quantify uncertainty. <em>Is it possible for a tool-augmented model to report a measure of certainty with its
          generated result?</em> We leave multi-modality and certainty quantification for future work.
      </p>
    </div>
  </section>









  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{theuma2024raven,
      title={Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance}, 
      author={Adrian Theuma and Ehsan Shareghi},
      year={2023},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->




  <footer class="footer">
    <!-- add three logo to the footer -->
    <div class="container is-max-desktop content">
      <div style="display: flex; justify-content: space-between;">
        <div style="width: 30%; margin-left: 20px; margin-right: 20px;">
          <a href="https://www.monash.edu/" target="_blank">
            <img src="static/images/monash-logo.png" alt="Monash University" width="100%">
          </a>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>