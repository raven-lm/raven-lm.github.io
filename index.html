<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/deb78776bf.js" crossorigin="anonymous"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Equipping Language Models with Tool Use Capability for Tabular Data
              Analysis in Finance</h1>
            <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Adrian Theuma,
              </span>
              <span class="author-block">
                <a href="https://eehsan.github.io" target="_blank">Ehsan Shareghi</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Monash University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary Model link -->
                <span class="link-block">
                  <a href="https://huggingface.co/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-regular fa-box-archive"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- Supplementary data link -->
                <span class="link-block">
                  <a href="https://github.com/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="item has-text-centered">
          <img src="static/images/Animation.gif" alt="Teaser image" class="teaser-image" width="100%">
        </div>
        <h2 class="subtitle has-text-centered">
          While language model fine-tuning and tool use are both popular topics, their use in specialised domains such
          as fianance remains understudied. This study initiates the exploration of numerous benefits associated with
          fine-tuning Language
          Models (LMs) for tool use in finance. Additionally, it raises new inquiries concerning the chanllenges of
          using language models in finance.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Despite impressive natural language generation capabilities exhibited by large language models (LLMs),
              their applicability in specialised domains is hindered by error propagation, hallucination, and the
              challenge of reasoning across heterogeneous datasets. These limitations pose significant obstacles in
              domains where precision is paramount, such as health and finance. We explore language model augmentation
              with external tools, which has shown promise in mitigating these limitations, by offloading specific steps
              of the reasoning process to external tools better equipped for the task.
              More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a
              LLaMA-2 13B Chat model to act both as a <em>task router</em> and <em>task solver</em>. The <em>task
                router</em> dynamically directs a question to either be answered internally by the LLM or externally via
              the
              right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of
              35.2% over the baseline, and is competitive with strong GPT-3.5 CoT and in-context learning baselines.
            </p>
          </div>
        </div>
      </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method -->
  <section class="section" id="method">
    <div class="container is-max-desktop content">
      <h2 class="title">Method</h2>
      <div class="item has-text-centered">
        <img src="static/images/raven-infeerence-pipeline.png" alt="Teaser image" class="teaser-image" width="90%">
      </div>
      <p>
        Inspired by the success of the <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a>
        instruction-tuning protocol we use the latest
        instruction-tuned <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">Llama 2 13B chat</a> language
        model and
        fine-tune it further using the <a href="https://huggingface.co/docs/peft/conceptual_guides/lora">LoRA</a>
        lightweight fine-tuning
        approach. For training we use a set of financial and generic structured and unstructured open-domain datasets to
        obtain a tool-augmented instruction following language model for finance, which we call Raven.
      </p>
      <p>
        <strong>Inference pipeline</strong>. We use different prompt templates to <em>steer</em> Raven to either produce
        the final answer or produce an intermediate result, which is subsequently interpreted by one of the external
        tools.
      </p>
      <p>
        <strong>Task-router.</strong> At inference time we prompt Raven twice for every query.
        Initially we
        wrap the prompt in a specialised <em>'template choice'</em> prompt and expect the model to predict the best
        prompt
        template to use from <em>'arithmetic'</em>, <em>'classification'</em>, <em>'script'</em> or <em>'information
          extraction'</em>,
        conditioned on the input.
      </p>
      <p>
        <strong>Task-solver.</strong> The instruction, including the input and data if applicable, are wrapped in the
        prompt template that the model inferred and this is sent to Raven to generate the next output.
        Depending on the selected template the <em>task router</em> then invokes a tool to fulfill the request, or
        produces the response directly.
      </p>

    </div>
  </section>

  <section class="section" id="tools">
    <div class="container is-max-desktop content">
      <h2 class="title">Tools</h2>
      Raven offloads nuanced structured
      data extraction and arithmetic expression calculation to two external deterministic tools: a calculator and a
      database engine.
      <p>
        <strong>Calculator API.</strong> A calculator is instantiated in a python interpreter and is used to evaluate
        well-formed arithmetic expressions. The API expects one parameter representing the arithmetic expression and
        returns the evaluated result.
      </p>
      <p>
        <strong>Lightweight database engine API.</strong> We create
        an API capable of executing SQL scripts on relational data. The API expects two parameters, (1) a string
        representation of the structured data in JSON format and (2) a SQL script. The API's internal lightweight
        database
        engine converts structured data from its textual form to the engine's internal relational representation and
        converts data types where applicable. The SQL script is executed on this internal representation and the API
        returns the resulting execution result.
      </p>

    </div>
  </section>


  <!--Result-->
  <section class="section" id="">
    <div class="container is-max-desktop content">
      <h2 class="title">Results</h2>
      <!DOCTYPE html>
      <html>

      <head>
        <style>
          table {
            border-collapse: collapse;
            width: 100%;
          }

          th,
          td {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 8px;
          }

          th {
            background-color: #f2f2f2;
          }
        </style>
      </head>

      <body>
        <table class="table is-hoverable">
          <thead>
            <tr class="header">
              <th></th>
              <th colspan="4" ;align="center">Dataset</th>
            </tr>
            <tr class="header">
              <th>Model</th>
              <th>OTT-QA</th>
              <th>TAT-QA</th>
              <th>Wiki-SQL</th>
              <th>PhraseBank</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT 3.5 (CoT)</td>
              <td>5.55%</td>
              <td>19.23%</td>
              <td>32.07%</td>
              <td>44.18%</td>
            </tr>
            <tr>
              <td>GPT 3.5 (5-Shot)</td>
              <td>14.55%</td>
              <td>34.06%</td>
              <td>53.00%</td>
              <td>70.07%</td>
            </tr>
            <tr>
              <td>&nbsp;&nbsp;+Tools</td>
              <td>14.60%</td>
              <td>46.82%</td>
              <td>75.88%</td>
              <td>71.73%</td>
            </tr>
            <tr>
              <td>Llama 2 13B chat</td>
              <td>6.18%</td>
              <td>10.91%</td>
              <td>21.68%</td>
              <td>66.03%</td>
            </tr>
            <tr>
              <td>&nbsp;&nbsp;+SFT</td>
              <td><strong>20.10%</strong></td>
              <td>37.87%</td>
              <td>74.38%</td>
              <td>90.97%</td>
            </tr>
            <tr>
              <td>Raven</td>
              <td>16.03%</td>
              <td>51.35%</td>
              <td>84.25%</td>
              <td><strong>91.92</strong>%</td>
            </tr>
            <tr>
              <td>&nbsp;&nbsp;+Backoff</td>
              <td>16.03%</td>
              <td><strong>52.27%</strong></td>
              <td><strong>85.52%</strong></td>
              <td>91.92%</td>
            </tr>
          </tbody>
        </table>


        <p><strong>Fine-tuning and using tools significantly improves performance.</strong> When compared to the base
          model Raven significantly improves the results on the PhraseBank
          dataset by an absolute 25.9%. These encouraging results suggest that
          even
          a limited dataset can endow a pre-trained language model with a sophisticated understanding of financial
          language, obtaining high accuracy in a financial sentiment classification benchmark.
        </p>
        <p>
        On the <a href="https://paperswithcode.com/dataset/wikisql">Wiki-SQL</a> dataset the base model is unable to infer the correct answer almost 80% of the
        time. This figure is inverted when the same benchmark is evaluated on Raven that obtains a
        <em>4-fold</em>> improvement over the base model inferring the correct answer more than 85% of the time. Our model
        improves on the best GPT-3.5 performance by close to 10% (absolute). All the questions in this dataset
        can be addressed using the lightweight database engine and involve a combination of
        data selection, ranking and arithmetic operations on structured data. This result underscores the distinct
        advantage of delegating this task to a tool rather than relying on the language model to infer the results in a
        zero-shot manner. Despite the results not being as strong as Raven we observe a similar pattern on the
        GPT-3.5 evaluation in which better results are incrementally obtained when including examples in the
        context and using tools compared to <a href="https://arxiv.org/abs/2205.11916">Zero-Shot-CoT</a>.
        </p>
        <p>

        We see a similar pattern on the <a href="https://paperswithcode.com/dataset/tat-qa">TAT-QA</a> benchmark with the tool augmented model achieving a
        <em>5-fold</em> improvement on the base model. Approximately 46% of the observations of the TAT-QA
        dataset are annotated with an intermediate arithmetic derivation that Raven evaluates using a
        calculator at inference time. In the analysis section we outline a comparative
        analysis to explore whether our model performs better on this partition of the data.
        </p>
<p>
        The majority of questions in <a href="https://paperswithcode.com/dataset/ott-qa">OTT-QA</a> require multi-hop inference involving both tabular data and
        unstructured text, with the information needed to answer the questions dispersed differently across these two
        input types. The baseline model from <a href="https://arxiv.org/abs/2010.10439">Chen at. al.</a>, which employs an iterative retriever and
        a BERT-based reader, attains an exact match score of less than 10%. This dataset does not have annotated
        intermediate steps to get to the answer and therefore all models are expected to infer the answer in a zero-shot
        manner without using tools. Despite Raven achieving an increase in experimental accuracy compared to
        the base model the relatively low score underscores the importance of intermediate reasoning steps and tools.
      </p>
      <p>

        <strong>Backoff.</strong> We employ a backoff mechanism when Raven produces malformed
        expressions and fallback to the result obtained from the SFT model. This mechanism improves the results of
        datasets that employ tools at inference time: TAT-QA (51.35% &rarr; 52.27%) and
        Wiki-SQL (84.25% &rarr; 85.52%).
      </p>



    </div>
  </section>



  <!-- Example -->
  <section class="section" id="example">
    <div class="container is-max-desktop content">
      <h2 class="title">Analysis</h2>
      <!-- insert two figures data_scale and data_type side by side-->
      <div class="columns is-centered">
        <div style="width: 30%;">
          <div class="item has-text-centered">
            <img src="static/images/data_scale.pdf" alt="Teaser image" class="teaser-image" width="90%">
          </div>
        </div>
        <div style="width: 70%;">
          <div class="item has-text-centered">
            <img src="static/images/data_type.pdf" alt="Teaser image" class="teaser-image" width="90%">
          </div>
        </div>
      </div>
      <p><strong>Effect of fine-tuning data scale.</strong> This analysis explores how <strong>FireAct</strong>
        performances scale with the number of fine-tuning trajectories (<em>n</em> in {100, 200, 500, 1000}). GPT-3.5
        appears very sample-efficient, requiring only 100 samples to reach an EM around 35, and the gain after 200
        samples is marginal. On the other hand, Llama models cannot even learn the <strong>ReAct</strong> format using
        100 or 200 samples, but non-trivial EMs "emerge" with 500 samples, and most models (except CodeLlama-13B)
        further improve with 1,000 samples.

      <div class="item has-text-centered">
        <img src="static/images/traj.pdf" alt="Teaser image" class="teaser-image" width="90%">
      </div>
      <p><strong>Multi-method fine-tuning increases agent flexibility.</strong> Before presenting quantitative results,
        we offer two example questions to illustrate the benefit of multi-method <strong>FireAct</strong> fine-tuning.
        The first question (a) is simple, but the <strong>ReAct</strong>-only fine-tuned agent (a1) searched for an
        over-complicated query, leading to distraction and a wrong answer. In contrast, an agent fine-tuned with both
        CoT and <strong>ReAct</strong> chose to solve the task within one round, relying on confident internal
        knowledge. The second question (b) is more challenging, and the <strong>ReAct</strong>-only fine-tuned agent
        (b1) kept searching queries ending in "during the Libyan Civil War" without useful information. In contrast, an
        agent fine-tuned with both Reflexion and <strong>ReAct</strong> reflected upon this problem and pivoted the
        search strategy to change the time constraint to "during his rule," which led to the right answer. The
        flexibility to implicitly choose methods for different problems is another key advantage of fine-tuning over
        prompting.</p>
      <p><strong>Multi-method fine-tuning affects different LMs differently.</strong> Despite the intuitive benefit,
        mixing more methods does not always improve results, and the optimal mix of methods depends on the base LM. For
        example, <strong>ReAct+CoT</strong> outperforms <strong>ReAct</strong> for GPT-3.5 and Llama-2 models, but hurts
        for CodeLlama models. <strong>ReAct+CoT+Reflexion</strong> is the worst for CodeLlama-7/13B, but is the best for
        CodeLlama-34B. These non-trivial results call for further studies of the interaction of base LMs and fine-tuning
        data.</p>
    </div>
    </div>
  </section>



  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{chen2023fireact,
      title={FireAct: Toward Language Agent Fine-tuning}, 
      author={Baian Chen and Chang Shu and Ehsan Shareghi and Nigel Collier and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2310.05915},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->




  <footer class="footer">
    <!-- add three logo to the footer -->
    <div class="container is-max-desktop content">
      <div style="display: flex; justify-content: space-between;">
        <div style="width: 30%; margin-left: 20px; margin-right: 20px;">
          <a href="https://www.monash.edu/" target="_blank">
            <img src="static/images/monash-logo.png" alt="Monash University" width="100%">
          </a>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>