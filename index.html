<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/deb78776bf.js" crossorigin="anonymous"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Equipping Language Models with Tool Use Capability for Tabular Data
              Analysis in Finance</h1>
            <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Adrian Theuma,
              </span>
              <span class="author-block">
                <a href="https://eehsan.github.io" target="_blank">Ehsan Shareghi</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Monash University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary Model link -->
                <span class="link-block">
                  <a href="https://huggingface.co/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-regular fa-box-archive"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- Supplementary data link -->
                <span class="link-block">
                  <a href="https://github.com/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="item has-text-centered">
          <img src="static/images/Animation.gif" alt="Teaser image" class="teaser-image" width="120%">
        </div>
        <h2 class="subtitle has-text-centered">
          While language model fine-tuning and tool use are both popular topics, their use in specialised domains such
          as fianance remains understudied. This study initiates the exploration of numerous benefits associated with
          fine-tuning Language
          Models (LMs) for tool use in finance. Additionally, it raises new inquiries concerning the chanllenges of
          using language models in finance.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Despite impressive natural language generation capabilities exhibited by large language models (LLMs),
              their applicability in specialised domains is hindered by error propagation, hallucination, and the
              challenge of reasoning across heterogeneous datasets. These limitations pose significant obstacles in
              domains where precision is paramount, such as health and finance. We explore language model augmentation
              with external tools, which has shown promise in mitigating these limitations, by offloading specific steps
              of the reasoning process to external tools better equipped for the task.
              More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a
              LLaMA-2 13B Chat model to act both as a <em>task router</em> and <em>task solver</em>. The <em>task
                router</em> dynamically directs a question to either be answered internally by the LLM or externally via
              the
              right tool from the tool set.
            </p>
          </div>
        </div>
      </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method -->
  <section class="section" id="method">
    <div class="container is-max-desktop content">
      <h2 class="title">Method</h2>
      <div class="item has-text-centered">
        <img src="static/images/raven-infeerence-pipeline.png" alt="Teaser image" class="teaser-image" width="90%">
      </div>
      <p>
        The FireAct framework includes two steps: <br>
        (a) <strong>During the fine-tuning</strong>, a robust language model (e.g., GPT-4) generates task-solving paths
        by analyzing questions from various datasets and employing diverse methods as prompts. These effective paths are
        then translated into the ReAct format to fine-tune a smaller language model (e.g. Llama2-7B).
        <br>
        (b) <strong>During inference</strong>, the fine-tuned language model can operate without the need for explicit
        prompts and has the capability to autonomously select an agent method, allowing it to complete ReAct
        trajectories with adaptable lengths, thus accommodating varying levels of question complexity. The example
        "3+4+5=" represents an ad-hoc question for illustration purposes.
      </p>
    </div>
    </div>
  </section>


  <!--Result-->
  <section class="section" id="">
    <div class="container is-max-desktop content">
      <h2 class="title">Results</h2>
      <!DOCTYPE html>
      <html>

      <head>
        <style>
          table {
            border-collapse: collapse;
            width: 100%;
          }

          th,
          td {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 8px;
          }

          th {
            background-color: #f2f2f2;
          }
        </style>
      </head>

      <body>

        <table>
          <tr>
            <td>
              <table>
                <caption>Few-shot Prompting results.</caption>
                <tr>
                  <th></th>
                  <th>Prompt</th>
                  <th>EM</th>
                </tr>
                <tr>
                  <td rowspan="3">GPT-4</td>
                  <td>IO</td>
                  <td>37.2</td>
                </tr>
                <tr>
                  <td>CoT</td>
                  <td><b>45.0</b></td>
                </tr>
                <tr>
                  <td>ReAct</td>
                  <td>42.0</td>
                </tr>
                <tr>
                  <td rowspan="3">GPT-3.5</td>
                  <td>IO</td>
                  <td>22.4</td>
                </tr>
                <tr>
                  <td>CoT</td>
                  <td>28.0</td>
                </tr>
                <tr>
                  <td>ReAct</td>
                  <td><b>31.4</b></td>
                </tr>
              </table>
            </td>
            <td>
              <table>
                <caption>Prompting vs. fine-tuning, with absolute/relative increases.</caption>
                <tr>
                  <th></th>
                  <th>ReAct</th>
                  <th>FireAct</th>
                  <th>abs./rel. diff</th>
                </tr>
                <tr>
                  <td>Llama-2-7B</td>
                  <td>14.8</td>
                  <td>26.2</td>
                  <td>+11.4 / <b>77%</b></td>
                </tr>
                <tr>
                  <td>Llama-2-13B</td>
                  <td><b>21.2</b></td>
                  <td>34.4</td>
                  <td><b>+13.1</b> / 62%</td>
                </tr>
                <tr>
                  <td>CodeLlama-7B</td>
                  <td>17.4</td>
                  <td>27.8</td>
                  <td>+10.4 / 60%</td>
                </tr>
                <tr>
                  <td>CodeLlama-13B</td>
                  <td>20.8</td>
                  <td>29.0</td>
                  <td>+8.2 / 39%</td>
                </tr>
                <tr>
                  <td>CodeLlama-34B</td>
                  <td><b>22.2</b></td>
                  <td>27.8</td>
                  <td>+5.6 / 25%</td>
                </tr>
                <tr>
                  <td>GPT-3.5</td>
                  <td><b>31.4</b></td>
                  <td><b>39.2</b></td>
                  <td>+7.8 / 25%</td>
                </tr>
              </table>
            </td>
          </tr>
        </table>

        <p><strong>Fine-tuning significantly improves agent performance.</strong> Lorem ipsum dolor sit amet,
          consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad
          minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute
          irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
          occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum..</p>




    </div>
  </section>



  <!-- Example -->
  <section class="section" id="example">
    <div class="container is-max-desktop content">
      <h2 class="title">Analysis</h2>
      <!-- insert two figures data_scale and data_type side by side-->
      <div class="columns is-centered">
        <div style="width: 30%;">
          <div class="item has-text-centered">
            <img src="static/images/data_scale.pdf" alt="Teaser image" class="teaser-image" width="90%">
          </div>
        </div>
        <div style="width: 70%;">
          <div class="item has-text-centered">
            <img src="static/images/data_type.pdf" alt="Teaser image" class="teaser-image" width="90%">
          </div>
        </div>
      </div>
      <p><strong>Effect of fine-tuning data scale.</strong> This analysis explores how <strong>FireAct</strong>
        performances scale with the number of fine-tuning trajectories (<em>n</em> in {100, 200, 500, 1000}). GPT-3.5
        appears very sample-efficient, requiring only 100 samples to reach an EM around 35, and the gain after 200
        samples is marginal. On the other hand, Llama models cannot even learn the <strong>ReAct</strong> format using
        100 or 200 samples, but non-trivial EMs "emerge" with 500 samples, and most models (except CodeLlama-13B)
        further improve with 1,000 samples.

      <div class="item has-text-centered">
        <img src="static/images/traj.pdf" alt="Teaser image" class="teaser-image" width="90%">
      </div>
      <p><strong>Multi-method fine-tuning increases agent flexibility.</strong> Before presenting quantitative results,
        we offer two example questions to illustrate the benefit of multi-method <strong>FireAct</strong> fine-tuning.
        The first question (a) is simple, but the <strong>ReAct</strong>-only fine-tuned agent (a1) searched for an
        over-complicated query, leading to distraction and a wrong answer. In contrast, an agent fine-tuned with both
        CoT and <strong>ReAct</strong> chose to solve the task within one round, relying on confident internal
        knowledge. The second question (b) is more challenging, and the <strong>ReAct</strong>-only fine-tuned agent
        (b1) kept searching queries ending in "during the Libyan Civil War" without useful information. In contrast, an
        agent fine-tuned with both Reflexion and <strong>ReAct</strong> reflected upon this problem and pivoted the
        search strategy to change the time constraint to "during his rule," which led to the right answer. The
        flexibility to implicitly choose methods for different problems is another key advantage of fine-tuning over
        prompting.</p>
      <p><strong>Multi-method fine-tuning affects different LMs differently.</strong> Despite the intuitive benefit,
        mixing more methods does not always improve results, and the optimal mix of methods depends on the base LM. For
        example, <strong>ReAct+CoT</strong> outperforms <strong>ReAct</strong> for GPT-3.5 and Llama-2 models, but hurts
        for CodeLlama models. <strong>ReAct+CoT+Reflexion</strong> is the worst for CodeLlama-7/13B, but is the best for
        CodeLlama-34B. These non-trivial results call for further studies of the interaction of base LMs and fine-tuning
        data.</p>
    </div>
    </div>
  </section>



  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{chen2023fireact,
      title={FireAct: Toward Language Agent Fine-tuning}, 
      author={Baian Chen and Chang Shu and Ehsan Shareghi and Nigel Collier and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2310.05915},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->




  <footer class="footer">
    <!-- add three logo to the footer -->
    <div class="container is-max-desktop content">
      <div style="display: flex; justify-content: space-between;">
        <div style="width: 30%; margin-left: 20px; margin-right: 20px;">
          <a href="https://www.monash.edu/" target="_blank">
            <img src="static/images/monash-logo.png" alt="Monash University" width="100%">
          </a>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>